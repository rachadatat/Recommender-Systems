{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gensim"
      ],
      "metadata": {
        "collapsed": true,
        "id": "VLmxTWRQ5XrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"numpy<2.0.0\" --force-reinstall"
      ],
      "metadata": {
        "id": "rKkLUqWh8RSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4uuYhqvr5Vcu"
      },
      "outputs": [],
      "source": [
        "#Importing the libraries\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity, manhattan_distances, euclidean_distances\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import re\n",
        "from gensim import models\n",
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.style\n",
        "%matplotlib inline\n",
        "from gensim.models import FastText as ft\n",
        "from IPython.display import Image\n",
        "import os\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download GloVe\n",
        "!wget https://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "\n",
        "# ------------------- 1. Load Pretrained Word Embeddings -------------------\n",
        "# Load GloVe embeddings\n",
        "import pandas as pd\n",
        "print(\"Loading GloVe Model...\")\n",
        "glove_df = pd.read_csv('glove.6B.300d.txt', sep=\" \",\n",
        "                       quoting=3, header=None, index_col=0)\n",
        "glove_model = {key: value.values for key, value in glove_df.T.items()}\n",
        "\n",
        "# Download the pre-trained model\n",
        "word2vecModel = api.load(\"word2vec-google-news-300\")"
      ],
      "metadata": {
        "id": "GogT685E931Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Sentence Embeddings\n",
        "In natural language processing (NLP), sentence embeddings are vector representations that capture the semantic meaning of a whole sentence. Unlike word embeddings, which represent individual words, sentence embeddings attempt to represent the meaning of an entire sentence or phrase.\n",
        "\n",
        "One straightforward method for generating sentence embeddings is by averaging the word embeddings of the words in the sentence. This method assumes that the meaning of a sentence can be approximated by combining the meanings of the individual words that it contains. Here's how the function get_sentence_vector() works to generate sentence embeddings:\n",
        "\n",
        "The function takes a sentence as input, tokenizes it into words, and looks up the corresponding word embeddings for each word in a pre-trained model (such as Word2Vec, GloVe, or FastText).\n",
        "The embeddings of the words are summed up, and the resulting vector is divided by the total number of words found in the model to get an average representation of the sentence.\n",
        "If any word in the sentence is not present in the embedding model, it is ignored, and the remaining words contribute to the final sentence embedding.\n",
        "This method provides a simple yet effective way to represent the semantic meaning of a sentence and can be used in various NLP tasks, including similarity comparison, clustering, or feeding into machine learning models for classification or regression.\n",
        "\n",
        "Below is the implementation of the function that generates a sentence embedding by averaging word vectors.\n",
        "\n",
        "Explore how changing the dimensionality of word embeddings (e.g., 50, 100, 300) affects the quality of recommendations."
      ],
      "metadata": {
        "id": "577XXUJwaW1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- Generate Sentence Embeddings -------------------\n",
        "def get_sentence_vector(sentence, model, embedding_size=300):\n",
        "    \"\"\"Get sentence embedding by averaging word vectors.\"\"\"\n",
        "    words = nltk.word_tokenize(sentence.lower())\n",
        "    sentence_vector = np.zeros((embedding_size,))\n",
        "    count = 0\n",
        "\n",
        "    for word in words:\n",
        "        if word in model:\n",
        "            sentence_vector += model[word]\n",
        "            count += 1\n",
        "\n",
        "    return sentence_vector / count if count > 0 else sentence_vector\n"
      ],
      "metadata": {
        "id": "E4vQ1wjAGKk_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-Processing"
      ],
      "metadata": {
        "id": "VmKbkHyradih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"Rec_sys_content.csv\", quotechar='\"', escapechar=\"\\\\\")\n",
        "\n",
        "# Extract the 3rd column (product description)\n",
        "product_descriptions = df.iloc[:, 2].dropna().astype(str).tolist()\n",
        "\n",
        "# Print a few descriptions to verify\n",
        "print(product_descriptions[:5])  # Print the first 5 descriptions\n"
      ],
      "metadata": {
        "id": "sV0JWv1PIRvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    # Remove the dollar sign and any other special characters if necessary\n",
        "    text = re.sub(r'\\$', '', text)  # Remove dollar signs\n",
        "    return text\n",
        "\n",
        "product_descriptions = [clean_text(description) for description in product_descriptions]"
      ],
      "metadata": {
        "id": "dy98crTQL5bz"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute Embeddings\n",
        "In this step, we compute the embeddings for each product description using pre-trained models like Word2Vec and GloVe. These models generate vector representations for words, and by averaging the word embeddings in a description, we obtain a vector that represents the entire product description. This enables us to measure the semantic similarity between different products based on their descriptions.\n",
        "\n",
        "Below is the code to compute the sentence embeddings for each product description using Word2Vec and GloVe:\n",
        "\n"
      ],
      "metadata": {
        "id": "dc5zTeN1andi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- Compute Embeddings -------------------\n",
        "word2vec_embeddings = np.array([get_sentence_vector(desc, word2vecModel) for desc in product_descriptions])\n",
        "glove_embeddings = np.array([get_sentence_vector(desc, glove_model) for desc in product_descriptions])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Q67npKYw885F"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute Similarity Scores\n",
        "This function calculates the similarity between a given product description and all other product descriptions using cosine similarity. It retrieves the most similar products based on their embeddings. To ensure diversity in the recommendations, it also checks for duplicate descriptions and excludes them from the top recommendations. The function then returns the top N most similar products based on their cosine similarity scores.\n",
        "\n",
        "Below is the code to compute and return the most similar products:"
      ],
      "metadata": {
        "id": "LnQDfr_lavfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- Compute Similarity Scores -------------------\n",
        "def get_most_similar(query_index, embeddings, descriptions, method=\"Embedding Model\", top_n=10):\n",
        "    \"\"\"Compute the most similar products based on cosine similarity.\"\"\"\n",
        "    similarities = cosine_similarity([embeddings[query_index]], embeddings)[0]\n",
        "    sorted_indices = np.argsort(similarities)[::-1][1:]  # Exclude itself\n",
        "\n",
        "    # Set to track unique descriptions\n",
        "    unique_descriptions = set()\n",
        "\n",
        "    # Select top_n most similar products\n",
        "    top_indices = []\n",
        "    top_similarities = []\n",
        "    for idx in sorted_indices:\n",
        "        if descriptions[idx] not in unique_descriptions:\n",
        "            unique_descriptions.add(descriptions[idx])\n",
        "            top_indices.append(idx)\n",
        "            top_similarities.append(similarities[idx])\n",
        "        if len(top_indices) == top_n:\n",
        "            break\n",
        "\n",
        "    print(f\"\\n🔹 Top {top_n} Recommendations for: '{descriptions[query_index]}' using {method}\")\n",
        "    for idx, similarity in zip(top_indices, top_similarities):\n",
        "        print(f\"   ✅ {descriptions[idx]} (Similarity: {similarity:.2f})\")\n",
        "\n",
        "    # Return the top_n recommendations' embeddings and descriptions\n",
        "    top_embeddings = embeddings[top_indices]\n",
        "    top_descriptions = [descriptions[idx] for idx in top_indices]\n",
        "    return top_embeddings, top_descriptions"
      ],
      "metadata": {
        "id": "fPJ9KJAQ9BXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task: Visualize the top 10 recommended embeddings in 2D using PCA (Principal Component Analysis).\n",
        "\n",
        "Instructions:\n",
        "\n",
        "1- Truncate Labels:\n",
        "\n",
        "The descriptions of the products can be long. To make the plot readable, truncate the labels to a maximum length (max_label_length).\n",
        "If the label exceeds this length, append ... at the end. If it doesn't exceed, keep the label as is.\n",
        "Apply PCA:\n",
        "\n",
        "2- Use PCA (Principal Component Analysis) to reduce the embeddings (which are high-dimensional vectors) into 2D space. This will help us visualize the embeddings in 2D.\n",
        "The method fit_transform() can be used to reduce the dimensionality of the embeddings to two components.\n",
        "Create a Scatter Plot:\n",
        "\n",
        "3- Use the plt.scatter() function to plot the 2D PCA components on the x and y axes.\n",
        "Each point on the scatter plot represents one of the top 10 recommended product embeddings.\n",
        "Annotate the Points:\n",
        "\n",
        "4- Annotate each point on the scatter plot with its truncated description so you can identify which product corresponds to each point.\n",
        "Use the plt.annotate() function for this. Ensure the annotations do not overlap by adjusting their position.\n",
        "Customize Plot:\n",
        "\n",
        "5- Add a title with plt.title(), and label the x and y axes with plt.xlabel() and plt.ylabel().\n"
      ],
      "metadata": {
        "id": "MBTUcfNCZH1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- Visualizing Top 10 Recommended Embeddings -------------------\n",
        "def plot_embeddings(embeddings, descriptions, title, max_label_length=30):\n",
        "    \"\"\"Visualize the embeddings of the top recommended products.\"\"\"\n",
        "\n",
        "    # 1. Truncate descriptions to avoid long labels\n",
        "    # (Fill in the missing line to truncate product descriptions)\n",
        "    truncated_labels = ____________  # Fill this line\n",
        "\n",
        "    # 2. Apply PCA (Principal Component Analysis)\n",
        "    # (Fill in the missing line to reduce embeddings to 2D)\n",
        "    pca = PCA(n_components=2)  # Initialize PCA to reduce to 2 components\n",
        "    reduced_embeddings = ____________  # Fill this line\n",
        "\n",
        "    # 3. Create the scatter plot\n",
        "    plt.figure(figsize=(10, 8))  # Increased figure size for better readability\n",
        "    plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], marker='o')\n",
        "\n",
        "    # 4. Annotate each point with the truncated description\n",
        "    for i, label in enumerate(truncated_labels):\n",
        "        plt.annotate(label, (reduced_embeddings[i, 0], reduced_embeddings[i, 1]), fontsize=9, ha='right', va='bottom')\n",
        "\n",
        "    # Customize the plot\n",
        "    plt.title(f\"{title} Top 10 Recommended Embeddings in 2D\")\n",
        "    plt.xlabel(\"PCA Component 1\")\n",
        "    plt.ylabel(\"PCA Component 2\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "pz5SEgC_ZGKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- Running for Multiple Models -------------------\n",
        "def process_and_plot(query_product, product_descriptions, embeddings_dict, top_n=10):\n",
        "    \"\"\"Process and plot the top recommendations for multiple models (Word2Vec, GloVe).\"\"\"\n",
        "    for method, embeddings in embeddings_dict.items():\n",
        "        print(f\"\\n📝 Running recommendations using {method} embeddings:\")\n",
        "        top_embeddings, top_descriptions = get_most_similar(query_product, embeddings, product_descriptions, method, top_n)\n",
        "        plot_embeddings(top_embeddings, top_descriptions, f\"{method} Recommendations\", max_label_length=50)\n",
        "\n",
        "\n",
        "embeddings_dict = {\n",
        "    \"Word2Vec\": word2vec_embeddings,\n",
        "    \"GloVe\": glove_embeddings\n",
        "}\n",
        "\n",
        "# Choose a product to base recommendations on\n",
        "query_product = 0  # Choose first product for example\n",
        "\n",
        "# Process and visualize for all models\n",
        "process_and_plot(query_product, product_descriptions, embeddings_dict, top_n=10)"
      ],
      "metadata": {
        "id": "57rSwPryZ5XJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}