{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gensim"
      ],
      "metadata": {
        "collapsed": true,
        "id": "VLmxTWRQ5XrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"numpy<2.0.0\" --force-reinstall"
      ],
      "metadata": {
        "id": "rKkLUqWh8RSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4uuYhqvr5Vcu"
      },
      "outputs": [],
      "source": [
        "#Importing the libraries\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity, manhattan_distances, euclidean_distances\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import re\n",
        "from gensim import models\n",
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.style\n",
        "%matplotlib inline\n",
        "from gensim.models import FastText as ft\n",
        "from IPython.display import Image\n",
        "import os\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download GloVe\n",
        "!wget https://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "\n",
        "# ------------------- 1. Load Pretrained Word Embeddings -------------------\n",
        "# Load GloVe embeddings\n",
        "import pandas as pd\n",
        "print(\"Loading GloVe Model...\")\n",
        "glove_df = pd.read_csv('glove.6B.300d.txt', sep=\" \",\n",
        "                       quoting=3, header=None, index_col=0)\n",
        "glove_model = {key: value.values for key, value in glove_df.T.items()}\n",
        "\n",
        "# Download the pre-trained model\n",
        "word2vecModel = api.load(\"word2vec-google-news-300\")"
      ],
      "metadata": {
        "id": "GogT685E931Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Sentence Embeddings\n",
        "In natural language processing (NLP), sentence embeddings are vector representations that capture the semantic meaning of a whole sentence. Unlike word embeddings, which represent individual words, sentence embeddings attempt to represent the meaning of an entire sentence or phrase.\n",
        "\n",
        "One straightforward method for generating sentence embeddings is by averaging the word embeddings of the words in the sentence. This method assumes that the meaning of a sentence can be approximated by combining the meanings of the individual words that it contains. Here's how the function get_sentence_vector() works to generate sentence embeddings:\n",
        "\n",
        "The function takes a sentence as input, tokenizes it into words, and looks up the corresponding word embeddings for each word in a pre-trained model (such as Word2Vec, GloVe, or FastText).\n",
        "The embeddings of the words are summed up, and the resulting vector is divided by the total number of words found in the model to get an average representation of the sentence.\n",
        "If any word in the sentence is not present in the embedding model, it is ignored, and the remaining words contribute to the final sentence embedding.\n",
        "This method provides a simple yet effective way to represent the semantic meaning of a sentence and can be used in various NLP tasks, including similarity comparison, clustering, or feeding into machine learning models for classification or regression.\n",
        "\n",
        "Below is the implementation of the function that generates a sentence embedding by averaging word vectors.\n",
        "\n",
        "Explore how changing the dimensionality of word embeddings (e.g., 50, 100, 300) affects the quality of recommendations."
      ],
      "metadata": {
        "id": "577XXUJwaW1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- Generate Sentence Embeddings -------------------\n",
        "def get_sentence_vector(sentence, model, embedding_size=300):\n",
        "    \"\"\"Get sentence embedding by averaging word vectors.\"\"\"\n",
        "    words = nltk.word_tokenize(sentence.lower())\n",
        "    sentence_vector = np.zeros((embedding_size,))\n",
        "    count = 0\n",
        "\n",
        "    for word in words:\n",
        "        if word in model:\n",
        "            sentence_vector += model[word]\n",
        "            count += 1\n",
        "\n",
        "    return sentence_vector / count if count > 0 else sentence_vector\n"
      ],
      "metadata": {
        "id": "E4vQ1wjAGKk_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-Processing"
      ],
      "metadata": {
        "id": "VmKbkHyradih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"Rec_sys_content.csv\", quotechar='\"', escapechar=\"\\\\\")\n",
        "\n",
        "# Extract the 3rd column (product description)\n",
        "product_descriptions = df.iloc[:, 2].dropna().astype(str).tolist()\n",
        "\n",
        "# Print a few descriptions to verify\n",
        "print(product_descriptions[:5])  # Print the first 5 descriptions\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sV0JWv1PIRvc",
        "outputId": "7942737c-5b84-4955-cca7-8c34f222c76f"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['New unique design, great gift.High quality plastic material.Clips over the back of the device to protect the back &amp; sides from bumps and scratches.Printed using a process called sublimation, high quality image which will last for years!| New unique design, great gift. High quality plastic material. Clips over the back of the device to protect the back &amp; sides from bumps and scratches. Printed using a process called sublimation, high quality image which will last for years! ', 'Rounded rectangular cat-eye reading glasses. These frames feature a deep burgundy color on the face of the frame with crystal interior, as well as crystal temple detail and yellow, turqoise and black marbled colors on the inside of the temple arms. The standard hinges provide durability and the TR-90 con struction allows for some flex making these glasses very sturdy and perfect for every day use!| Always custom made to your individual order specifications in our FDA Listed Lab with the remarkable Optical grade Custom frames and Lenses available. Better style better grade and always less than retail. Rx lens included! Glasses come with a Hard protective case and micro fiber cleaning cloth. Very light and strong frame with Ebe signature comfort fit bridge for easy all day use without hot spots. We design and produce all of our frames and lenses in our FDA listed Labs. Lenses Include Anti-Glare and Anti-Reflective Coating. You will Love your New Glasses and we Guarantee it 100%. We do not sell Cheap &quot;drug store&quot; reading glasses. Lens Width=54, Lens Height=37, Bridge Width=15, Temple Length=144, Frame Width=135 ', \"Each Nintendo 2DS kit is printed with super-high resolution graphics with a ultra finish. All skins are protected with MightyShield. This laminate protects from scratching, fading, peeling and most importantly leaves no sticky mess guaranteed. Our patented advanced air-release vinyl guarantees a perfect installation everytime. When you are ready to change your skin removal is a snap, no sticky mess or gooey residue for over 4 years. This package is a 2 piece vinyl skin kit. You can't go wrong with a MightySkin.|This is NOT A HARD CASE (Skin Decal Only). It is a vinyl skin/decal sticker and is NOT made of rubber, silicone, gel or plastic. Device shown not included.\", 'The sheerest compression stocking in its class is exactly what is states: sheer and soft. mediven sheer &amp; soft,which is virtually indistinguishable from regular hosiery, is specifically designed for fashion-conscious patients who demand the most durable, best-looking medical compression available. Ideal for both day and evening wear, only you and your patients will know that mediven sheer &amp; soft is a medical compression stocking.| Unequaled sheerness and softness Machine wash and tumble dry Easy to apply ', 'Features: -Made in the USA. -Sawtooth hanger on the back for easy and instant hanging. -Hand-finished. Product Type: -Wall Letters/Monogram. Theme: -Letters. Gender: -Neutral. Life Stage: -Kid. Primary D&eacute;cor Material: -Wood. Pattern: -Chevron. Hand Painted: -Yes. Holiday / Occasion: -No Holiday. Dimensions: Overall Height - Top to Bottom: -18&quot;. Overall Width - Side to Side: -0.5&quot;. Overall Product Weight: -2 lbs. Overall Thickness: -12&quot;.|VYH1295 Features Made in the USA Sawtooth hanger on the back for easy and instant hanging Hand-finished Product Type: Wall Letters/Monogram Theme: Letters Gender: Neutral Life Stage: Kid Primary D&eacute;cor Material: Wood Pattern: Chevron Hand Painted: Yes Holiday / Occasion: No Holiday Dimensions Overall Height - Top to Bottom: 18&quot; Overall Width - Side to Side: 0.5&quot; Overall Product Weight: 2 lbs Overall Thickness: 12&quot; ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    # Remove the dollar sign and any other special characters if necessary\n",
        "    text = re.sub(r'\\$', '', text)  # Remove dollar signs\n",
        "    return text\n",
        "\n",
        "product_descriptions = [clean_text(description) for description in product_descriptions]"
      ],
      "metadata": {
        "id": "dy98crTQL5bz"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute Embeddings\n",
        "In this step, we compute the embeddings for each product description using pre-trained models like Word2Vec and GloVe. These models generate vector representations for words, and by averaging the word embeddings in a description, we obtain a vector that represents the entire product description. This enables us to measure the semantic similarity between different products based on their descriptions.\n",
        "\n",
        "Below is the code to compute the sentence embeddings for each product description using Word2Vec and GloVe:\n",
        "\n"
      ],
      "metadata": {
        "id": "dc5zTeN1andi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- Compute Embeddings -------------------\n",
        "word2vec_embeddings = np.array([get_sentence_vector(desc, word2vecModel) for desc in product_descriptions])\n",
        "glove_embeddings = np.array([get_sentence_vector(desc, glove_model) for desc in product_descriptions])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Q67npKYw885F"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute Similarity Scores\n",
        "This function calculates the similarity between a given product description and all other product descriptions using cosine similarity. It retrieves the most similar products based on their embeddings. To ensure diversity in the recommendations, it also checks for duplicate descriptions and excludes them from the top recommendations. The function then returns the top N most similar products based on their cosine similarity scores.\n",
        "\n",
        "Below is the code to compute and return the most similar products:"
      ],
      "metadata": {
        "id": "LnQDfr_lavfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- Compute Similarity Scores -------------------\n",
        "def get_most_similar(query_index, embeddings, descriptions, method=\"Embedding Model\", top_n=10):\n",
        "    \"\"\"Compute the most similar products based on cosine similarity.\"\"\"\n",
        "    similarities = cosine_similarity([embeddings[query_index]], embeddings)[0]\n",
        "    sorted_indices = np.argsort(similarities)[::-1][1:]  # Exclude itself\n",
        "\n",
        "    # Set to track unique descriptions\n",
        "    unique_descriptions = set()\n",
        "\n",
        "    # Select top_n most similar products\n",
        "    top_indices = []\n",
        "    top_similarities = []\n",
        "    for idx in sorted_indices:\n",
        "        if descriptions[idx] not in unique_descriptions:\n",
        "            unique_descriptions.add(descriptions[idx])\n",
        "            top_indices.append(idx)\n",
        "            top_similarities.append(similarities[idx])\n",
        "        if len(top_indices) == top_n:\n",
        "            break\n",
        "\n",
        "    print(f\"\\nüîπ Top {top_n} Recommendations for: '{descriptions[query_index]}' using {method}\")\n",
        "    for idx, similarity in zip(top_indices, top_similarities):\n",
        "        print(f\"   ‚úÖ {descriptions[idx]} (Similarity: {similarity:.2f})\")\n",
        "\n",
        "    # Return the top_n recommendations' embeddings and descriptions\n",
        "    top_embeddings = embeddings[top_indices]\n",
        "    top_descriptions = [descriptions[idx] for idx in top_indices]\n",
        "    return top_embeddings, top_descriptions"
      ],
      "metadata": {
        "id": "fPJ9KJAQ9BXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task: Visualize the top 10 recommended embeddings in 2D using PCA (Principal Component Analysis).\n",
        "\n",
        "Instructions:\n",
        "\n",
        "1- Truncate Labels:\n",
        "\n",
        "The descriptions of the products can be long. To make the plot readable, truncate the labels to a maximum length (max_label_length).\n",
        "If the label exceeds this length, append ... at the end. If it doesn't exceed, keep the label as is.\n",
        "Apply PCA:\n",
        "\n",
        "2- Use PCA (Principal Component Analysis) to reduce the embeddings (which are high-dimensional vectors) into 2D space. This will help us visualize the embeddings in 2D.\n",
        "The method fit_transform() can be used to reduce the dimensionality of the embeddings to two components.\n",
        "Create a Scatter Plot:\n",
        "\n",
        "3- Use the plt.scatter() function to plot the 2D PCA components on the x and y axes.\n",
        "Each point on the scatter plot represents one of the top 10 recommended product embeddings.\n",
        "Annotate the Points:\n",
        "\n",
        "4- Annotate each point on the scatter plot with its truncated description so you can identify which product corresponds to each point.\n",
        "Use the plt.annotate() function for this. Ensure the annotations do not overlap by adjusting their position.\n",
        "Customize Plot:\n",
        "\n",
        "5- Add a title with plt.title(), and label the x and y axes with plt.xlabel() and plt.ylabel().\n"
      ],
      "metadata": {
        "id": "MBTUcfNCZH1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- Visualizing Top 10 Recommended Embeddings -------------------\n",
        "def plot_embeddings(embeddings, descriptions, title, max_label_length=30):\n",
        "    \"\"\"Visualize the embeddings of the top recommended products.\"\"\"\n",
        "\n",
        "    # 1. Truncate descriptions to avoid long labels\n",
        "    # (Fill in the missing line to truncate product descriptions)\n",
        "    truncated_labels = ____________  # Fill this line\n",
        "\n",
        "    # 2. Apply PCA (Principal Component Analysis)\n",
        "    # (Fill in the missing line to reduce embeddings to 2D)\n",
        "    pca = PCA(n_components=2)  # Initialize PCA to reduce to 2 components\n",
        "    reduced_embeddings = ____________  # Fill this line\n",
        "\n",
        "    # 3. Create the scatter plot\n",
        "    plt.figure(figsize=(10, 8))  # Increased figure size for better readability\n",
        "    plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], marker='o')\n",
        "\n",
        "    # 4. Annotate each point with the truncated description\n",
        "    for i, label in enumerate(truncated_labels):\n",
        "        plt.annotate(label, (reduced_embeddings[i, 0], reduced_embeddings[i, 1]), fontsize=9, ha='right', va='bottom')\n",
        "\n",
        "    # Customize the plot\n",
        "    plt.title(f\"{title} Top 10 Recommended Embeddings in 2D\")\n",
        "    plt.xlabel(\"PCA Component 1\")\n",
        "    plt.ylabel(\"PCA Component 2\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "pz5SEgC_ZGKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- Running for Multiple Models -------------------\n",
        "def process_and_plot(query_product, product_descriptions, embeddings_dict, top_n=10):\n",
        "    \"\"\"Process and plot the top recommendations for multiple models (Word2Vec, GloVe).\"\"\"\n",
        "    for method, embeddings in embeddings_dict.items():\n",
        "        print(f\"\\nüìù Running recommendations using {method} embeddings:\")\n",
        "        top_embeddings, top_descriptions = get_most_similar(query_product, embeddings, product_descriptions, method, top_n)\n",
        "        plot_embeddings(top_embeddings, top_descriptions, f\"{method} Recommendations\", max_label_length=50)\n",
        "\n",
        "\n",
        "embeddings_dict = {\n",
        "    \"Word2Vec\": word2vec_embeddings,\n",
        "    \"GloVe\": glove_embeddings\n",
        "}\n",
        "\n",
        "# Choose a product to base recommendations on\n",
        "query_product = 0  # Choose first product for example\n",
        "\n",
        "# Process and visualize for all models\n",
        "process_and_plot(query_product, product_descriptions, embeddings_dict, top_n=10)"
      ],
      "metadata": {
        "id": "57rSwPryZ5XJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}